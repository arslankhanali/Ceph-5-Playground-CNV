#########################################################
# Great Guide: https://knowledgebase.45drives.com/kb/kb450422-configuring-ceph-object-storage-to-use-multiple-data-pools/
# check all rules
ceph osd crush rule ls

#########################################################
# Define Variable
export ecprofile=ecprofile
export ecpool=ecpool
export rpool=rpool
export app=rgw
export zonegroup=lahore
export zone=east

#########################################################
# ERASURE CODING
# check ec profiles
ceph osd erasure-code-profile ls

# Create ec profile
ceph osd erasure-code-profile set $ecprofile \
    k=2 \
    m=1 \
    crush-failure-domain=host

# Create ec pool
ceph osd pool create $ecpool erasure $ecprofile
# Tag pool with rgw application
ceph osd pool application enable $ecpool $app

# Note that for the newly created erasure-coded pool, the MAX AVAIL column shows a higher number of bytes compared with the replicated pool because of the lower raw-to-usable storage ratio.
ceph df

#########################################################
# REPLICATION
# Create pool
ceph osd pool create $rpool 32 32 replicated_rule 3 
# Tag pool with rgw application
ceph osd pool application enable $rpool $app

#########################################################
# UPDATE ZONEGROUP
radosgw-admin zonegroup list
radosgw-admin zonegroup get > zonegroup.json
# radosgw-admin zonegroup get  --rgw-zonegroup=$zonegroup > $zonegroup.json

# Edit zonegroup.json to add a 2 placment targets. 
# Under
# "placement_targets": [
#         {
#             "name": "default-placement",
#             "tags": [],
#             "storage_classes": [
#                 "STANDARD"
#             ]
#         },
# Name them anything
# I will name mine ec-placement and r-placement
radosgw-admin zonegroup set --infile zonegroup.json

# UPDATE ZONE
radosgw-admin zone get > zone.json
# Edit zone.json to add the 2 placment targets. 
# Under
# "placement_pools": [
#         {
#             "key": "default-placement",
#             "val": {
#                 "index_pool": "east.rgw.buckets.index",
#                 "storage_classes": {
#                     "STANDARD": {
#                         "data_pool": "east.rgw.buckets.data"
#                     }
#                 },
#                 "data_extra_pool": "east.rgw.buckets.non-ec",
#                 "index_type": 0
#             }
#         },
# Name 'keys' according to the zonegroup names you gave before i.e. ec-placement and r-placement
# Then just change data_pool to pool names you created before 'ecpool' and 'rpool'
radosgw-admin zone set --infile zone.json

# Restart rgw on nodes
ceph orch ps
ceph orch daemon restart rgw.east.ceph-mon01.ijgaeh

cd cephadm-ansible/ && ansible -i /etc/ansible/hosts all -l cluster1 -m shell -a "sudo systemctl  restart *rgw*" 

# Commit Changes [VERY IMPORTANT When you have multi-site replication setup] 
radosgw-admin period update --commit

# DO THE SAME ON SECONDARY CLUSTER
radosgw-admin period update --commit
#########################################################
#  Lets test with SWIFT CLI
# On Cluster 1
# Install swift cli
pip3 install python-swiftclient

# Create a user
radosgw-admin user create --uid='user1' --display-name='First User' --access-key='S3user1' --secret-key='S3user1key'
radosgw-admin subuser create --uid='user1' --subuser='user1:swift' --secret-key='Swiftuser1key' --access=full

# Set variables
export IP="localhost"
export PORT="80"
export SWIFT_AUTH_URL="http://$IP:$PORT/auth/1.0"
export SWIFT_USER="user1:swift"
export SWIFT_KEY="Swiftuser1key"
export R_BUCKET_NAME="replication_bucket"
export EC_BUCKET_NAME="ec_bucket"
export DUMMY_NAME="dummy_file"

# Create 300Mb dumy object
base64 /dev/urandom | head -c 30000000 > $DUMMY_NAME

# Check conenction by listing all buckets to this user
swift -A $SWIFT_AUTH_URL -U $SWIFT_USER -K "$SWIFT_KEY" list

# Create Replication bucket
swift -A $SWIFT_AUTH_URL -U $SWIFT_USER -K "$SWIFT_KEY" post -H "X-Storage-Policy: r-placement" $R_BUCKET_NAME
# Create EC bucket
swift -A $SWIFT_AUTH_URL -U $SWIFT_USER -K "$SWIFT_KEY" post -H "X-Storage-Policy: ec-placement" $EC_BUCKET_NAME

# Upload to Rep bucket
swift -A $SWIFT_AUTH_URL -U $SWIFT_USER -K "$SWIFT_KEY" upload $R_BUCKET_NAME $DUMMY_NAME
# Upload to EC bucket
swift -A $SWIFT_AUTH_URL -U $SWIFT_USER -K "$SWIFT_KEY" upload $EC_BUCKET_NAME $DUMMY_NAME

# List objects in buckets
swift -A $SWIFT_AUTH_URL -U $SWIFT_USER -K "$SWIFT_KEY" list $R_BUCKET_NAME
swift -A $SWIFT_AUTH_URL -U $SWIFT_USER -K "$SWIFT_KEY" list $EC_BUCKET_NAME

#########################################################
# Explanation
ceph df

--- RAW STORAGE ---
CLASS    SIZE   AVAIL     USED  RAW USED  %RAW USED
hdd    60 GiB  57 GiB  3.1 GiB   3.1 GiB       5.18
TOTAL  60 GiB  57 GiB  3.1 GiB   3.1 GiB       5.18
 
--- POOLS ---
POOL                     ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr                      1    1  449 KiB        2  1.3 MiB      0     18 GiB
east.rgw.otp              6   32      0 B        0      0 B      0     18 GiB
east.rgw.log              7   32   51 KiB      665  3.8 MiB      0     18 GiB
.rgw.root                 8   32  8.5 KiB       19  216 KiB      0     18 GiB
default.rgw.log           9   32    182 B        2   24 KiB      0     18 GiB
default.rgw.control      10   32      0 B        8      0 B      0     18 GiB
default.rgw.meta         11   32      0 B        0      0 B      0     18 GiB
east.rgw.control         12   32      0 B        8      0 B      0     18 GiB
east.rgw.meta            13   32   10 KiB       20  211 KiB      0     18 GiB
east.rgw.buckets.index   14   32   11 KiB       77   33 KiB      0     18 GiB
east.rgw.buckets.data    15  256  206 MiB       61  618 MiB   1.12     18 GiB
east.rgw.buckets.non-ec  16   32  4.5 KiB        0   13 KiB      0     18 GiB
ecpool                   17   32   29 MiB        9   43 MiB   0.08     36 GiB
rpool                    21   32   29 MiB        8   86 MiB   0.16     18 GiB

# Explanation
Total available Storage = 57 GiB
Since most pools are replication(3x) = Their usable storage is 57/3 ~ 18Gb

Notice because ecpool is using [2,1] Erasure coding profile it has more available storage
(2/(2+1))*57 ~ 36Gb

#########################################################
# Find out on which OSDs was the objects saved
# Check which OSD is on what host
# Since crush-failure-domain=host, Object should not be on OSDs which are on the same host
ceph osd tree
ceph osd map $rpool $DUMMY_NAME -f json-pretty
ceph osd map $ecpool $DUMMY_NAME -f json-pretty